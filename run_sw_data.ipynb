{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "import os\n",
    "import torch\n",
    "import pdb\n",
    "import json\n",
    "from copy import copy\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "# from collections.abc import Mapping\n",
    "\n",
    "from main_loop import main_loop, prediction, main_parser\n",
    "from models import ClassificationReasoner\n",
    "from buffer import Buffer\n",
    "from utils import CAPACITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Please confirm the 20news data are ready by ./20news/process_20news.py!')\n",
    "print('=====================================')\n",
    "root_dir = os.path.abspath(os.path.dirname(__file__))\n",
    "parser = ArgumentParser(add_help=False)\n",
    "# ------------ add dataset-specific argument ----------\n",
    "parser.add_argument('--reasoner_config_num_labels', type=int, default=20)\n",
    "parser.add_argument('--only_predict', action='store_true')\n",
    "# ---------------------------------------------\n",
    "parser = main_parser(parser) # 加上其他一大堆parser (在main_loop.py裡面) (上面兩個是專門為資料集而設置的 也就是用不同資料集會有不同的initial parser 之後再加上general的parser)\n",
    "parser.set_defaults(\n",
    "    train_source = os.path.join('c:\\\\vs_code_python', 'data', '20news_train.pkl'),\n",
    "    test_source = os.path.join('c:\\\\vs_code_python', 'data', '20news_test.pkl')\n",
    ")\n",
    "config = parser.parse_args()\n",
    "config.reasoner_cls_name = 'ClassificationReasoner' # 這個任務是要做分類\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "\n",
    "# def conditional_trans_classification(qbuf, dbuf):\n",
    "#     assert len(qbuf) == 1\n",
    "#     new_qbuf = Buffer()\n",
    "#     new_qblk = copy(qbuf[0])\n",
    "#     new_qblk.ids = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(new_qblk.label_name.replace('.', ' ')))\n",
    "#     new_qbuf.blocks.append(new_qblk)\n",
    "#     return new_qbuf, dbuf\n",
    "# config.conditional_transforms = [conditional_trans_classification]\n",
    "\n",
    "if not config.only_predict: # train \n",
    "    main_loop(config)\n",
    "\n",
    "ans, acc, total, acc_long, total_long = {}, 0., 0, 0., 0\n",
    "for qbuf, dbuf, buf, relevance_score, ids, output in prediction(config):\n",
    "    _id = qbuf[0]._id\n",
    "    pred, gold = output[0].view(-1).argmax().item(), int(qbuf[0].label)\n",
    "    ans[_id] = (pred, gold)\n",
    "    total += 1.\n",
    "    acc += pred == gold\n",
    "    if dbuf.calc_size() + 2 > CAPACITY:\n",
    "        acc_long += pred == gold\n",
    "        total_long += 1\n",
    "        # if pred != gold:\n",
    "        #     import pdb; pdb.set_trace()\n",
    "acc /= total\n",
    "acc_long /= total_long\n",
    "print(f'accuracy: {acc}')\n",
    "print(f'for long text: accuray {acc_long}, total {total_long}')\n",
    "with open(os.path.join(config.tmp_dir, 'pred_20news.json'), 'w') as fout:\n",
    "    json.dump(ans, fout)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
